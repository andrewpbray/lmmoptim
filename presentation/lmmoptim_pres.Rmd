---
title: "Approximately Exact Calculations for LMM"
author: "Andrew Bray, UMass Amherst"
date: "April 15, 2015"
output:
  ioslides_presentation:
    logo: umass_seal.png
---


## Where we're going

A method to evaluate the likelihood or posterior for a linear mixed model in order to

1. **Profile** the shape of the function for sensible inference
2. **Estimate** parameters

while being sure that we're not missing the global optimum.


## Genesis

*Approximately Exact Calculations for Linear Mixed Models*, Lavine and Hodges, 2015.


## Linear Mixed Models

\[ 
y = X \beta + Z u + \epsilon
\]

- $y$: vector of $n$ observations
- $X$: known $n \times p$ matrix
- $\beta$: vector of $p$ unknown fixed effects coefs
- $Z$: $n \times q$ matrix
- $u$: vector of $q$ unknown random effects coefs
- $\epsilon$: vector of $n$ errors

\[
\epsilon \sim \textrm{N}(0, \sigma^2_e \Sigma_e); \quad u \sim \textrm{N}(0, \sigma^2_s \Sigma_s)
\]

## The Objective Functions

**Log restricted likelihood**

\[
\log \textrm{RL} (\sigma^2_e, \sigma^2_s) = B - \frac{n_e}{2}\log(\sigma^2_e) - \frac{y'\Gamma_c \Gamma_c' y}{2 \sigma^2_e} - \frac{1}{2} \sum_j \left[ \log(a_j \sigma^2_s + \sigma^2_e) + \frac{\hat{v}^2_j}{a_j \sigma^2_s + \sigma^2_e} \right]
\]

**Log posterior**

\[
\log \pi (\sigma^2_e, \sigma^2_s) = B - \frac{1}{2}\sum_j \left[ c_j \log (a_j \sigma^2_s + b_j \sigma^2_e ) \frac{d_j}{a_j \sigma^2_s + b_j \sigma^2_e} \right]
\]

## Existing Methods in `lmer()`

1. BOBYQA: quadratic approximation
2. Nelder-Mead: iterative simplex

<div class="centered">
<img src="http://upload.wikimedia.org/wikipedia/commons/9/96/Nelder_Mead2.gif" width="300px" />
</div>

**Pros**: works for any $f$, derivative-free, fast

**Cons**: can miss global optimum


# The algorithm

## Linear Structure

\[
\log \pi (\sigma^2_e, \sigma^2_s) = B - \frac{1}{2}\sum_j \left[ c_j \log (a_j \sigma^2_s + b_j \sigma^2_e ) \frac{d_j}{a_j \sigma^2_s + b_j \sigma^2_e} \right]
\]


## Example

## Comparison to lmer
- is M needlessley complicated?
- why does v get a hat?

